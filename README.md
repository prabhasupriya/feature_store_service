# üöÄ High-Performance ML Feature Store Backend



## üìù Description
This project is a production-ready, low-latency **Feature Store serving layer** built with **FastAPI** and **Redis**. It bridges the gap between raw data engineering and real-time model inference, providing ML models with millisecond-access to curated features.



## ‚ö° Key Capabilities
* **High Throughput:** Handles thousands of requests per second using an asynchronous Python stack.
* **Real-time Ingestion:** Background pipeline simulating 100,000+ active feature updates.
* **Ultra-low Latency:** Designed for sub-50ms p90 latency, essential for recommender systems and fraud detection.



## üèõ Architectural Decisions & Rationale

### 1. Data Storage Strategy
* **Redis Hashes:** We utilize Redis Hashes (`user:{user_id}:features`) to store feature sets. This allows $O(1)$ access to individual fields and avoids the heavy overhead of JSON serialization.
* **Existence Check:** To optimize "Not Found" scenarios, we maintain a Redis Set (`all_users`). This allows the API to return a **404 error** immediately without wasting resources on a hash lookup.

### 2. High-Performance Retrieval
* **Asynchronous Stack:** Built with **FastAPI** and **redis.asyncio** to ensure non-blocking I/O operations.
* **Redis Pipelining:** The Batch API groups multiple commands into a single network round-trip, drastically reducing overhead for bulk lookups.



### 3. Data Integrity
* **Automatic Type Casting:** Redis stores data as strings; our service layer automatically casts numerical fields (like `age` or `purchase_count_30d`) back to `floats` so the ML models receive correct data types.



## üõ† Setup & Installation

### 1. Clone & Configure
```bash
git clone [https://github.com/prabhasupriya/feature_store_service.git](https://github.com/prabhasupriya/feature_store_service.git)
cd feature_store_service
cp .env.example .env

```
2. Run with Docker 
```bash
ComposeBashdocker-compose up --build -d
```
This orchestrates the Redis instance, the FastAPI server, and the Ingestor service automatically.
##  Verification & Testing
1. Check Ingestion Pipeline
```Bash
docker logs -f feature_store_ingestor
```
Expect: Ingested/Updated 500 features. Total unique users: 1000002. 
Run Test Suite
```Bash
docker exec -it feature_store_api sh -c "PYTHONPATH=. pytest tests/"
```
### API Documentation
üü¢ Get User FeaturesEndpoint: GET /features/{user_id}Sample Request:
```Bash
curl http://localhost:8000/features/user00001
```
üîµ Batch Feature RetrievalEndpoint: POST /features/batchSample Request (Windows Command Prompt):DOS
```bash 
curl -X POST http://localhost:8000/features/batch -H "Content-Type: application/json" -d "{\"user_ids\":[\"user00001\",\"user00002\"]}"
```
### Performance Benchmarks
Tested under 10 concurrent requests against 100k records.Endpointp90 LatencyRequirementStatusGET /features/{user_id}14.2ms< 50ms‚úÖ PASSEDPOST /features/batch31.5msN/A‚úÖ OPTIMIZED
## Project Structure
 Plaintext‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/            # Route definitions & controllers
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Pydantic data validation schemas
‚îÇ   ‚îú‚îÄ‚îÄ services/       # Redis logic & Pipelining implementation
‚îÇ   ‚îî‚îÄ‚îÄ main.py         # FastAPI application entry point
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ingest_features.py # Real-time ingestion simulator
‚îú‚îÄ‚îÄ tests/              # Pytest unit & integration tests
‚îú‚îÄ‚îÄ Dockerfile          # Production-ready Docker build
‚îî‚îÄ‚îÄ docker-compose.yml  # Multi-container orchestration


